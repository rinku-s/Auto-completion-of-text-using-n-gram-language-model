{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Auto-completion of text using N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Auto-completion (sometimes known as \"word prediction\") refers to the prediction of words typed by a user. It has various applications including web browsers, search engines, email programs, word processors, etc. It helps in increasing the typing speed of the user thus saving time and boosting productivity.\n",
    "\n",
    "A Language Model is a probability distribution over a set of word sequences. These models rely on training data which contain text corpora of large, structured text sequences that can help in finding the probability of a word given a particular sequence preceding it. A Language Model can help predict the next word based on its probability given a sequence of previous words.\n",
    "\n",
    "This paper focuses on the implementation of the statistical n-gram model for calculating the probability of the next word for implementing auto-completion given a previous sequence of words.\n",
    "This paper aims to achieve the following objectives -\n",
    "\n",
    "1. Choosing a Dataset and Obtaining Data\n",
    "   * Import Libraries\n",
    "   * Load the dataset\n",
    "2. Pre-processing data\n",
    "   * Split the data into sentences\n",
    "   * Tokenize the sentences into words\n",
    "3. Splitting the data into train & test sets\n",
    "4. Text Normalization Step\n",
    "    * Find the top 10 most frequent words in the set\n",
    "    * Generate a closed vocabulary list\n",
    "    * Normalize the train set with \\<UNK\\> token\n",
    "5. Building the n-gram model\n",
    "    * Why n-gram model?\n",
    "    * Estimation of probabilities\n",
    "    * Smoothing\n",
    "6. Testing the n-gram model\n",
    "    * Input of test sequence\n",
    "    * Input of first letter of output (Optional)\n",
    "    * n-gram frequency matrix\n",
    "    * n-gram probability matrix\n",
    "    * Word prediction    \n",
    "7. Evaulating the n-gram model\n",
    "   * Calculating the Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part A & B: Choosing a Dataset, Obtaining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The dataset chosen for this notebook is the Twitter Dataset (**https://www.kaggle.com/datasets/crmercado/tweets-blogs-news-swiftkey-dataset-4million**) containing more than 6 million words in a text file in string form. This dataset was chosen because it provides a large corpus of structured text to form sequences of words and adequate for division into train and test sets. The text file is easy to obtain from the website and is included in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Importing Libraries & Loading the Dataset**\n",
    "Import Python libraries pandas and numpy to organize data and perform analysis. Import src directory containing functions that help in loading, pre-processing, prediction and evaulation of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.data.path.append('.')\n",
    "import sys\n",
    "import os\n",
    "import src\n",
    "import src.preproc as preproc\n",
    "import src.traintestsplit as traintestsplit\n",
    "import src.wordvocabulary as wordvocabulary\n",
    "import src.ngram as ngram\n",
    "import src.perplexity as perplexity\n",
    "import src.prediction as prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The dataset is loaded by opening the text file and reading its contents in the preproc file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Twitter Data...\n",
      "Twitter data loaded successfully...\n",
      "\n",
      "Data type: <class 'str'>\n",
      "\n",
      "There are 47961 tweets in the dataset.\n",
      "There are 616296 words in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Twitter Data...\")\n",
    "tweets = preproc.load_data()\n",
    "number_of_tweets = tweets.count('\\n')\n",
    "if (tweets !=\"\"):\n",
    "    print(\"Twitter data loaded successfully...\")\n",
    "    print(\"\\nData type:\", type(tweets))\n",
    "    print(\"\\nThere are\", number_of_tweets, \"tweets in the dataset.\")\n",
    "    number_of_words = len(tweets.split())\n",
    "    print(\"There are \" + str(number_of_words) + \" words in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part C: Pre-processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data loaded from the Twitter dataset requires some pre-processing before it can be used for sentence auto-completion.\n",
    "\n",
    "### Split the data into sentences\n",
    "\n",
    "First, the data is split into individual sentences and trimmed to remove whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets split succesfully...\n"
     ]
    }
   ],
   "source": [
    "sentences = preproc.splitSentences(tweets)\n",
    "if (sentences!=\"\"):\n",
    "    print(\"Tweets split succesfully...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The sentences are displayed using a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TWEETS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they've decided its more fun if I don't.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So Tired D; Played Lazer Tag &amp; Ran A LOT D; Ughh Going To Sleep Like In 5 Minutes ;)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Words from a complete stranger! Made my birthday even better :)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                            TWEETS\n",
       "0    How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\n",
       "1  When you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\n",
       "2                                                                         they've decided its more fun if I don't.\n",
       "3                             So Tired D; Played Lazer Tag & Ran A LOT D; Ughh Going To Sleep Like In 5 Minutes ;)\n",
       "4                                                  Words from a complete stranger! Made my birthday even better :)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.DataFrame(sentences, columns=['TWEETS'])\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenize the sentences into words\n",
    "\n",
    "Next, the sentences are all changed to lowercase and split into words. NLTK's punkt tokenizer is used for this purpose. The result which is a list of words for each tweet is stored in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweets tokenized successfully\n",
      "\n",
      "Tweet before tokenization:\n",
      " How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\n",
      "Tweet after tokenization:\n",
      " ['how', 'are', 'you', '?', 'btw', 'thanks', 'for', 'the', 'rt', '.', 'you', 'gon', 'na', 'be', 'in', 'dc', 'anytime', 'soon', '?', 'love', 'to', 'see', 'you', '.', 'been', 'way', ',', 'way', 'too', 'long', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = preproc.tokenizeSentences(sentences)\n",
    "if (len(tokenized_sentences) !=0):\n",
    "    print(\"\\nTweets tokenized successfully\")\n",
    "    print(\"\\nTweet before tokenization:\\n\", sentences[0])\n",
    "    print(\"Tweet after tokenization:\\n\", tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part D: Split data into train & test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After pre-processing the data, the data can be split into train set and test set. Here, we keep 80% of the data as train data and 20% as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into train and test set successfully...\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = traintestsplit.splitData(tokenized_sentences)\n",
    "if (len(train_data) != 0 and len(test_data)):\n",
    "    print(\"Data split into train and test set successfully...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set contains 38368 sentences\n",
      "Test set contains 9593 sentences\n"
     ]
    }
   ],
   "source": [
    "print (\"Train set contains \" + str(len(train_data) )+ \" sentences\")\n",
    "print (\"Test set contains \" + str(len(test_data)) + \" sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Part E:Text Normalization Step**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A collection of the most frequently occurring words is also known as **closed vocabulary**. The n-gram model can predict the probabilities of the words in test set that are a part of the closed vocabulary list. \n",
    "\n",
    "However, there may be words in the test set that do not appear in the closed vocabulary created from the training set. In this case, a mechanism is required to deal with the new or \"unknown\" words. \n",
    "\n",
    "For this purpose, a count threshold **N** is introduced. N is an arbitary small integer depending on which a word may be included or excluded from the closed vocabulary of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the top 10 most frequent words in the set\n",
    "In the dataset, the frequency of all words is counted and the top 10 most frequent words are listed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Frequent Words in the Dataset -\n",
      "\n",
      "Word\tCount\n",
      "-------------\n",
      "the\t19036\n",
      "i\t18500\n",
      "to\t15787\n",
      "a\t12370\n",
      "you\t12282\n",
      "and\t8840\n",
      "it\t7879\n",
      "for\t7801\n",
      "is\t7709\n",
      "in\t7678\n"
     ]
    }
   ],
   "source": [
    "word_keys =[]\n",
    "word_counts = wordvocabulary.countWords(tokenized_sentences)\n",
    "word_counts_keys = sorted(word_counts, key=word_counts.get, reverse=True)[:20]\n",
    "type(word_counts_keys)\n",
    "for word in word_counts_keys:\n",
    "    if word.isalnum():\n",
    "        word_keys.append(word)\n",
    "print(\"\\nTop 10 Most Frequent Words in the Dataset -\")\n",
    "print(\"\\nWord\\tCount\")\n",
    "print(\"-------------\")\n",
    "for word in word_keys[:10]:\n",
    "    print(word + \"\\t\" + str(word_counts.get(word)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generate a closed vocabulary list\n",
    "\n",
    "If a word appears at least N or more than N times, then it is included in the closed vocabulary. Now, create a list of closed vocabulary words in the train set with a count threshold value N=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 14782 in the closed vocabulary.\n"
     ]
    }
   ],
   "source": [
    "high_freq_words = wordvocabulary.getHighFreqWords (train_data, 2)\n",
    "if len(high_freq_words)!=0:\n",
    "    print(\"\\nThere are \" + str(len(high_freq_words)) + \" in the closed vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Normalize the train set with \\<UNK\\> token\n",
    "\n",
    "Further, the rest of the words are **\"Out of Vocabulary (OOV)\"** words. These words have a low frequency in the train set. To deal with OOV words which comprise of low frequency words in the train set and the new words not from the test set, the **\\<UNK\\>** token  which indicates unknown words is introduced. The OOV words in the train set are replaced with the token. Then the token is processed by the n-gram model as a regular word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding <UNK> token\n",
      "Original Train Data\n",
      "[['i', \"'m\", 'sorry', '.'], ['cause', 'yo', 'junky', 'ass', 'always', 'got', 'change', 'pussy', '!', '...']]\n",
      "Original Test Data\n",
      "[['just', 'got', 'to', 'intelligentsia', '-', 'free', 'wifi', ',', 'yea', '!', 'still', 'setting', 'up', ',', 'and', 'i', 'am', 'in', 'the', 'middle', 'of', 'the', 'shop', 'with', 'notebook', 'and', 'webcam', '!'], ['everyone', 'eventually', 'leaves', ',', 'willingly', 'it', 'unwillingly', '.']]\n",
      "-------------------------------\n",
      "After adding <UNK> token\n",
      "Modified Train Data\n",
      "[['i', \"'m\", 'sorry', '.'], ['cause', 'yo', '<UNK>', 'ass', 'always', 'got', 'change', 'pussy', '!', '...']]\n",
      "Modified Test Data\n",
      "[['just', 'got', 'to', '<UNK>', '-', 'free', 'wifi', ',', 'yea', '!', 'still', 'setting', 'up', ',', 'and', 'i', 'am', 'in', 'the', 'middle', 'of', 'the', 'shop', 'with', 'notebook', 'and', 'webcam', '!'], ['everyone', 'eventually', 'leaves', ',', '<UNK>', 'it', '<UNK>', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before adding <UNK> token\")\n",
    "train_data_sample_before = train_data[:2]\n",
    "test_data_sample_before = test_data[0:2]\n",
    "print(\"Original Train Data\")\n",
    "print(train_data_sample_before)\n",
    "print(\"Original Test Data\")\n",
    "print(test_data_sample_before)\n",
    "train_data_processed = wordvocabulary.addUnkownToken(train_data, high_freq_words)\n",
    "test_data_processed = wordvocabulary.addUnkownToken(test_data, high_freq_words)\n",
    "print(\"-------------------------------\")\n",
    "print(\"After adding <UNK> token\")\n",
    "print(\"Modified Train Data\")\n",
    "print(train_data_processed[0:2])\n",
    "print(\"Modified Test Data\")\n",
    "print(test_data_processed[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Part F: Building the n-gram model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "n-gram simply refers to a sequence of **n** words or characters from a given sentence. n-gram is a classic probabilistic language model assumes that the probability of the next word in a sequence is dependent on previous (n-1) words. The base of this assumption is the  n<sup>th</sup> order Markov assumption. In a Markov model the assumption is that we can forecast the likelihood of a future item in a sequence without considering items too far back in the sequence. For a uni-gram model, it is 1<sup>st</sup> order Markov assumption. Similarly, for bi-gram model, it is 2<sup>nd</sup> order Markov assumption. \n",
    "\n",
    "\n",
    "**Why n-gram model?**\n",
    "- It is a simple model to understand and implement.\n",
    "- Higher values of n which contain more context with a reasonable space-time tradeoff allow the usage of this model in a small project which may expand later.\n",
    "\n",
    "For a given previous sequence of words $w_{i-1}, w_{i-2} \\cdots w_{i-n}$ for a word at i<sup>th</sup> position, the conditional probability of word $w_{i}$ can be given as -\n",
    "\n",
    "$$ P(w_i | w_{i-1}\\dots w_{i-n}) \\tag{1}$$\n",
    "\n",
    "This probability is nothing but the frequency or count of the given sequence in the train set.\n",
    "\n",
    "**Estimation of probabilities**\n",
    "\n",
    "The probabilities for an n-gram can be estimated by **maximum likelihood estimation (MLE)** which involves counting the frequency of the n-gram in the corpus and then normalizing it (dividing by  total count) to fall between 0 and 1. In other words, n-gram probability for a word i in a sequence of words  i, i-1, ..., i-n is a ratio of the frequency of a sequence of words from word 'i' through i-n to frequency of sequence of words i-1 through i-n occur in the train set. The equation is given as -\n",
    "\n",
    "$$ \\hat{P}(w_i | w_{i-1}\\dots w_{i-n}) = \\frac{C(w_{i-1}\\dots w_{i-n}, w_n)}{C(w_{i-1}\\dots w_{i-n})} \\tag{2} $$\n",
    "\n",
    "where $C(\\cdots)$ function is denotes the number of times of the given sequence occurs.\n",
    "and $\\hat{P}$ denotes estimation of $P$.\n",
    "\n",
    "For example, for a certain train set we have the following data -\n",
    "\n",
    "- Test data: \"I like tea more than\"\n",
    "- Expected output: \"coffee\"\n",
    "\n",
    "So, the probability of the word \"coffee\" occurring after the given test sequence and value of n as 5 is calculated as \n",
    "\n",
    "P(\"coffee\") = Number of times \"I like tea more than coffee\" in train set **/** Number of times \"I like tea more than\" in train set\n",
    "\n",
    "So n-grams probability estimation requires the counts of (n+1)-grams for numerator and the counts of n-grams for denominator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code, n-grams are calculated for the train data with n value ranging from 1 to 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing n-gram counts with n = 1 ...\n",
      "Computing n-gram counts with n = 2 ...\n",
      "Computing n-gram counts with n = 3 ...\n",
      "Computing n-gram counts with n = 4 ...\n",
      "Computing n-gram counts with n = 5 ...\n",
      "Finished computing n-gram counts\n"
     ]
    }
   ],
   "source": [
    "n_gram_counts_list = []\n",
    "# Computing n-gram counts with n in (1,2,3,4,5)\n",
    "for n in range(1, 6):\n",
    "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = ngram.countNGrams(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)\n",
    "print(\"Finished computing n-gram counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Smoothing**\n",
    "\n",
    "It is noted that this equation would work for a sequence which already occurs in the train set. However, there may be cases where an existing word in the train set is used in a different context in the test set and the expected output does not exist in the train set after the test input (an unknown n-gram). This will render the above probability for it as 0. To avoid this problem, a technique known as **smoothing** or **discounting** is introduced. Smoothing involves assigning some probability to such unseen events. The smoothing technique used in this notebook is **add-k smoothing**. Here, a fraction k which is constant is added to count of n-grams of the numerator and k x |V| is added to the denominator. For unknown n-grams, the probability now becomes 1/|V|. The equation with add-k smoothing is given below -\n",
    "\n",
    "$$ \\hat{P}(w_i | w_{i-1}\\dots w_{i-n}) = \\frac{C(w_{i-1}\\dots w_{i-n}, w_n) + k}{C(w_{i-1}\\dots w_{i-n}) + k|V|} \\tag{3} $$\n",
    "\n",
    "where k = constant\n",
    "\n",
    "and |V| = the number of words in the vocabulary of the train set.\n",
    "\n",
    "The disadvantage of smoothing is that it may assign higher probabilities to unknown n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part G: Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, it is time to test the model created.\n",
    "\n",
    "### **Input of test sequence**\n",
    "\n",
    "The user inputs a sentence by running the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a sentence : \n",
      "i would like to\n",
      "['i', 'would', 'like', 'to']\n"
     ]
    }
   ],
   "source": [
    "input_sentence = input(\"Please enter a sentence : \\n\")\n",
    "# sentences = preproc.split_to_sentences(input_sentence)\n",
    "tokenized_input = nltk.word_tokenize(input_sentence)\n",
    "# tokenized_input = get_tokenized_data(input_sentence)\n",
    "print(tokenized_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Input of first letter of output (Optional)**\n",
    "\n",
    "Then subsequently user may enter the first letter of the next word optionally. If the user does not want to enter the first letter, they can skip this by pressing 'Enter' in the text box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the first letter of the next word: \n",
      "k\n",
      "Computing n-gram counts with n = 1 ...\n",
      "Computing n-gram counts with n = 2 ...\n",
      "Computing n-gram counts with n = 3 ...\n",
      "Computing n-gram counts with n = 4 ...\n"
     ]
    }
   ],
   "source": [
    "next_word_starts_with = input(\"Please enter the first letter of the next word: \\n\")\n",
    "n_gram_counts_list = []\n",
    "for n in range(1, len(tokenized_input) + 1):\n",
    "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = ngram.countNGrams(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)\n",
    "unique_words = list(set(tokenized_input))\n",
    "word_prediction = prediction.getWordPredictions(tokenized_input, n_gram_counts_list, high_freq_words, k=0.05,  start_with=next_word_starts_with)\n",
    "\n",
    "word_prediction_list = list(map(list, word_prediction))\n",
    "\n",
    "for predicted_word in word_prediction_list:\n",
    "    tmp_probability = np.format_float_positional(predicted_word[1], trim='-')\n",
    "    predicted_word[1] = tmp_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **n-gram frequency matrix**\n",
    "\n",
    "A matrix containing n-grams and their frequences for trigram (n=3) is displayed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>to</th>\n",
       "      <th>would</th>\n",
       "      <th>i</th>\n",
       "      <th>like</th>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <th>&lt;UNK&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(&lt;S&gt;, i)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like, to)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(would, like)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;S&gt;, &lt;S&gt;)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i, would)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                to  would    i  like  <END>  <UNK>\n",
       "(<S>, i)       0.0    1.0  0.0   0.0    0.0    0.0\n",
       "(like, to)     0.0    0.0  0.0   0.0    1.0    0.0\n",
       "(would, like)  1.0    0.0  0.0   0.0    0.0    0.0\n",
       "(<S>, <S>)     0.0    0.0  1.0   0.0    0.0    0.0\n",
       "(i, would)     0.0    0.0  0.0   1.0    0.0    0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "sentences.append(tokenized_input)\n",
    "unique_words = list(set(sentences[0]))\n",
    "unigram_counts = ngram.countNGrams(sentences, 1)\n",
    "bigram_counts = ngram.countNGrams(sentences, 2)\n",
    "trigram_counts = ngram.countNGrams(sentences, 3)\n",
    "ngram.generateCountMatrix(trigram_counts, unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **n-gram probabilities matrix**\n",
    "\n",
    "A matrix containing the probability of each word and the n-gram for trigram (n=3) is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>to</th>\n",
       "      <th>would</th>\n",
       "      <th>i</th>\n",
       "      <th>like</th>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <th>&lt;UNK&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(&lt;S&gt;, i)</th>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like, to)</th>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(would, like)</th>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;S&gt;, &lt;S&gt;)</th>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i, would)</th>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     to     would         i      like     <END>     <UNK>\n",
       "(<S>, i)       0.038462  0.807692  0.038462  0.038462  0.038462  0.038462\n",
       "(like, to)     0.038462  0.038462  0.038462  0.038462  0.807692  0.038462\n",
       "(would, like)  0.807692  0.038462  0.038462  0.038462  0.038462  0.038462\n",
       "(<S>, <S>)     0.038462  0.038462  0.807692  0.038462  0.038462  0.038462\n",
       "(i, would)     0.038462  0.038462  0.038462  0.807692  0.038462  0.038462"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram.generateProbabilities([\"<s>\", \"<s>\"], bigram_counts, trigram_counts, unique_words, k=0.05)\n",
    "ngram.generateProbabilityMatrix(trigram_counts, unique_words, k=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word Prediction**\n",
    "\n",
    "Finally, a list of words predicted for auto-complete are displayed along in order of their calculated probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Next word</th>\n",
       "      <th>Probability</th>\n",
       "      <th>N-gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>know</td>\n",
       "      <td>0.007786891380161949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>know</td>\n",
       "      <td>0.0036000944287063264</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>keep</td>\n",
       "      <td>0.00006491820306413918</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Next word             Probability  N-gram\n",
       "0      know    0.007786891380161949       1\n",
       "1      know   0.0036000944287063264       2\n",
       "2      keep  0.00006491820306413918       3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_prediction_df = pd.DataFrame(word_prediction_list, columns=['Next word', 'Probability'])\n",
    "word_prediction_df['N-gram'] = range(1, len(word_prediction_df) + 1)\n",
    "word_prediction_df.sort_values('Probability', inplace=True, ascending=False)\n",
    "word_prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part H: Evaluating the n-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Models are evaluated through a metric known as **Perplexity (PP)**. The perplexity of a language model on a test set is the test set's inverse probability normalised by the number of words. As inverse is taken, the lower the perplexity, the higher the conditional probability of the word sequence.\n",
    "Perplexity can be calculated using the formula given below -\n",
    "\n",
    "\\begin{equation}\n",
    " PP(W)= \\sqrt[N]{\\prod_{i=1}^{N}{\\frac{1}{P(w_i|w_{i-1})}}}\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "test set W = w1, w2, ...wN,\n",
    "\n",
    "length of the sentence = N\n",
    "\n",
    "### **Calculating Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity calculated: 1.1177\n"
     ]
    }
   ],
   "source": [
    "unigram_counts = ngram.countNGrams(sentences, 1)\n",
    "bigram_counts = ngram.countNGrams(sentences, 2)\n",
    "unique_words = set(tokenized_input)\n",
    "perplexity_test = perplexity.getPerplexity(tokenized_input,\n",
    "                                       unigram_counts, bigram_counts,\n",
    "                                       len(unique_words), k=0.05)\n",
    "print(f\"Perplexity calculated: {perplexity_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, minimising perplexity is equivalent to maximising test set probability of a language model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
